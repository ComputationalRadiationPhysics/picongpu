/**
 * Copyright 2013-2016 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "pmacc_types.hpp"
#include "particles/frame_types.hpp"

#include "simulation_defines.hpp"

#include "FieldJ.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"


#include "algorithms/Velocity.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"

#include "particles/frame_types.hpp"

namespace picongpu
{

using namespace PMacc;

typedef FieldJ::DataBoxType J_DataBox;

template< int workerMultiplier, class BlockDescription_, uint32_t AREA >
struct KernelComputeCurrent
{
    template<class JBox, class ParBox, class Mapping, class FrameSolver>
    DINLINE void operator()(JBox fieldJ,
                                         ParBox boxPar, FrameSolver frameSolver, Mapping mapper) const
    {
        typedef typename ParBox::FrameType FrameType;
        typedef typename ParBox::FramePtr FramePtr;
        typedef typename Mapping::SuperCellSize SuperCellSize;

        const uint32_t cellsPerSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;

        const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
        const DataSpace<simDim > threadIndex(threadIdx);

        /* thread id, can be greater than cellsPerSuperCell*/
        const int linearThreadIdx = DataSpaceOperations<simDim>::template map<SuperCellSize > (threadIndex);

        const uint32_t virtualBlockId = linearThreadIdx / cellsPerSuperCell;
        /* move linearThreadIdx for all threads to [0;cellsPerSuperCell) */
        const int virtualLinearId = linearThreadIdx - (virtualBlockId * cellsPerSuperCell);


        FramePtr frame;
        lcellId_t particlesInSuperCell = 0;

        frame = boxPar.getLastFrame(block);
        if (frame.isValid() && virtualBlockId == 0)
            particlesInSuperCell = boxPar.getSuperCell(block).getSizeLastFrame();

        /* select N-th (N=virtualBlockId) frame from the end of the list*/
        for (int i = 1; (i <= virtualBlockId) && frame.isValid(); ++i)
        {
            particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
            frame = boxPar.getPreviousFrame(frame);
        }

        /* This memory is used by all virtual blocks*/
        PMACC_AUTO(cachedJ, CachedBox::create < 0, typename JBox::ValueType > (BlockDescription_()));

        Set<typename JBox::ValueType > set(float3_X::create(0.0));
        ThreadCollective<BlockDescription_, cellsPerSuperCell * workerMultiplier> collectiveSet(linearThreadIdx);
        collectiveSet(set, cachedJ);

        __syncthreads();

        while (frame.isValid())
        {
            /* this test is only importend for the last frame
             * if frame is not the last one particlesInSuperCell==particles count in supercell
             */
            if (virtualLinearId < particlesInSuperCell)
            {
                frameSolver(*frame,
                            virtualLinearId,
                            cachedJ);
            }

            particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
            for (int i = 0; (i < workerMultiplier) && frame.isValid(); ++i)
            {
                frame = boxPar.getPreviousFrame(frame);
            }
        }

        /* we wait that all threads finish the loop*/
        __syncthreads();

        nvidia::functors::Add add;
        const DataSpace<simDim> blockCell = block * SuperCellSize::toRT();
        ThreadCollective<BlockDescription_, cellsPerSuperCell * workerMultiplier> collectiveAdd(linearThreadIdx);
        PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));
        collectiveAdd(add, fieldJBlock, cachedJ);
    }
};

template<class ParticleAlgo, class Velocity, class TVec>
struct ComputeCurrentPerFrame
{

    HDINLINE ComputeCurrentPerFrame(const float_X deltaTime) :
    m_deltaTime(deltaTime)
    {
    }

    template<class FrameType, class BoxJ >
    DINLINE void operator()(FrameType& frame, const int localIdx, BoxJ & jBox)
    {

        PMACC_AUTO(particle, frame[localIdx]);
        const float_X weighting = particle[weighting_];
        const floatD_X pos = particle[position_];
        const int particleCellIdx = particle[localCellIdx_];
        const float_X charge = attribute::getCharge(weighting,particle);
        const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec > (particleCellIdx));

        Velocity velocity;
        const float3_X vel = velocity(
                                      particle[momentum_],
                                      attribute::getMass(weighting,particle));
        PMACC_AUTO(fieldJShiftToParticle, jBox.shift(localCell));
        ParticleAlgo perParticle;
        perParticle(fieldJShiftToParticle,
                    pos,
                    vel,
                    charge,
                    m_deltaTime
                    );
    }

private:
    const PMACC_ALIGN(m_deltaTime, float_32);
};

struct KernelAddCurrentToEMF
{
    template<class T_CurrentInterpolation, class T_Mapping>
    DINLINE void operator()(typename FieldE::DataBoxType fieldE,
                                          typename FieldB::DataBoxType fieldB,
                                          J_DataBox fieldJ,
                                          T_CurrentInterpolation currentInterpolation,
                                          T_Mapping mapper) const
    {
        /* Caching of fieldJ */
        typedef SuperCellDescription<
                    SuperCellSize,
                    typename T_CurrentInterpolation::LowerMargin,
                    typename T_CurrentInterpolation::UpperMargin
                    > BlockArea;

        PMACC_AUTO(cachedJ, CachedBox::create < 0, typename J_DataBox::ValueType > (BlockArea()));

        nvidia::functors::Assign assign;
        const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
        const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();

        const DataSpace<simDim > threadIndex(threadIdx);
        PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));

        ThreadCollective<BlockArea> collective(threadIndex);
        collective(
                  assign,
                  cachedJ,
                  fieldJBlock
                  );

        __syncthreads();

        const DataSpace<T_Mapping::Dim> cell(blockCell + threadIndex);

        // Amperes Law:
        //   Change of the dE = - j / EPS0 * dt
        //                        j = current density (= current per area)
        //                          = fieldJ
        currentInterpolation( fieldE.shift(cell), fieldB.shift(cell), cachedJ.shift(threadIndex) );
    }
};

struct KernelBashCurrent
{
    template<class Mapping>
    DINLINE void operator()(J_DataBox fieldJ,
                                      J_DataBox targetJ,
                                      DataSpace<simDim> exchangeSize,
                                      DataSpace<simDim> direction,
                                      Mapping mapper) const
    {
        const DataSpace<simDim> blockCell(
                                          mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                          * Mapping::SuperCellSize::toRT()
                                          );
        const DataSpace<Mapping::Dim> threadIndex(threadIdx);
        const DataSpace<Mapping::Dim> sourceCell(blockCell + threadIndex);

        /*origin in area from local GPU*/
        DataSpace<simDim> nullSourceCell(
                                         mapper.getSuperCellIndex(DataSpace<simDim > ())
                                         * Mapping::SuperCellSize::toRT()
                                         );
        DataSpace<simDim> targetCell(sourceCell - nullSourceCell);

        for (uint32_t d = 0; d < simDim; ++d)
        {
            if (direction[d] == -1)
            {
                if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
                targetCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
            }
            else if ((direction[d] == 1) && (threadIndex[d] >= exchangeSize[d])) return;
        }

        targetJ(targetCell) = fieldJ(sourceCell);
    }
};

struct KernelInsertCurrent
{
    template<class Mapping>
    DINLINE void operator()(J_DataBox fieldJ,
                                        J_DataBox sourceJ,
                                        DataSpace<simDim> exchangeSize,
                                        DataSpace<simDim> direction,
                                        Mapping mapper) const
    {
        const DataSpace<Mapping::Dim> threadIndex(threadIdx);
        const DataSpace<simDim> blockCell(
                                          mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                          * Mapping::SuperCellSize::toRT()
                                          );
        DataSpace<Mapping::Dim> targetCell(blockCell + threadIndex);

        /*origin in area from local GPU*/
        DataSpace<simDim> nullSourceCell(
                                         mapper.getSuperCellIndex(DataSpace<simDim > ())
                                         * Mapping::SuperCellSize::toRT()
                                         );
        DataSpace<simDim> sourceCell(targetCell - nullSourceCell);

        for (uint32_t d = 0; d < simDim; ++d)
        {
            if (direction[d] == 1)
            {
                if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
                sourceCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
                targetCell[d] -= SuperCellSize::toRT()[d];
            }
            else if (direction[d] == -1)
            {
                if (threadIndex[d] >= exchangeSize[d]) return;
                targetCell[d] += SuperCellSize::toRT()[d];
            }
        }

        fieldJ(targetCell) += sourceJ(sourceCell);
    }
};

}
