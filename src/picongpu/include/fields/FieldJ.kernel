/**
 * Copyright 2013-2014 Axel Huebl, Heiko Burau, Rene Widera
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "types.h"
#include "particles/frame_types.hpp"
#include "basicOperations.hpp"

#include "simulation_defines.hpp"

#include "FieldJ.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"


#include "algorithms/Velocity.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"

#include "particles/frame_types.hpp"

namespace picongpu
{

using namespace PMacc;

typedef typename FieldJ::DataBoxType J_DataBox;

template<int workerMultiplier, class BlockDescription_, uint32_t AREA, class JBox, class ParBox, class Mapping, class FrameSolver>
__global__ void kernelComputeCurrent(JBox fieldJ,
                                     ParBox boxPar, FrameSolver frameSolver, Mapping mapper)
{
    typedef typename ParBox::FrameType FrameType;
    typedef typename Mapping::SuperCellSize SuperCellSize;

    const uint32_t cellsPerSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;

    const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
    const DataSpace<simDim > threadIndex(threadIdx);

    /* thread id, can be greater than cellsPerSuperCell*/
    const int linearThreadIdx = DataSpaceOperations<simDim>::template map<SuperCellSize > (threadIndex);

    const uint32_t virtualBlockId = linearThreadIdx / cellsPerSuperCell;
    /* move linearThreadIdx for all threads to [0;cellsPerSuperCell) */
    const int virtualLinearId = linearThreadIdx - (virtualBlockId * cellsPerSuperCell);


    FrameType* frame = NULL;
    bool isValid = false;
    lcellId_t particlesInSuperCell = 0;

    frame = &(boxPar.getLastFrame(block, isValid));
    if (isValid && virtualBlockId == 0)
        particlesInSuperCell = boxPar.getSuperCell(block).getSizeLastFrame();

    /* select N-th (N=virtualBlockId) frame from the end of the list*/
    for (int i = 1; (i <= virtualBlockId) && isValid; ++i)
    {
        particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
        frame = &(boxPar.getPreviousFrame(*frame, isValid));
    }

    /* This memory is used by all virtual blocks*/
    PMACC_AUTO(cachedJ, CachedBox::create < 0, typename JBox::ValueType > (BlockDescription_()));

    __syncthreads();

    Set<typename JBox::ValueType > set(float3_X(0.0, 0.0, 0.0));
    ThreadCollective<BlockDescription_, cellsPerSuperCell * workerMultiplier> collectivSet(linearThreadIdx);
    collectivSet(set, cachedJ);

    __syncthreads();

    while (isValid)
    {
        /* this test is only importend for the last frame
         * if frame is not the last one particlesInSuperCell==particles count in supercell
         */
        if (virtualLinearId < particlesInSuperCell)
        {
            frameSolver(*frame,
                        virtualLinearId,
                        cachedJ);
        }

        particlesInSuperCell = PMacc::math::CT::volume<SuperCellSize>::type::value;
        for (int i = 0; (i < workerMultiplier) && isValid; ++i)
        {
            frame = &(boxPar.getPreviousFrame(*frame, isValid));
        }
    }

    /* we wait that all threads finish the loop*/
    __syncthreads();

    nvidia::functors::Add add;
    const DataSpace<simDim> blockCell = block * SuperCellSize::toRT();
    ThreadCollective<BlockDescription_, cellsPerSuperCell * workerMultiplier> collectivAdd(linearThreadIdx);
    PMACC_AUTO(fieldJBlock, fieldJ.shift(blockCell));
    collectivAdd(add, fieldJBlock, cachedJ);
}

template<class ParticleAlgo, class Velocity, class TVec>
struct ComputeCurrentPerFrame
{

    HDINLINE ComputeCurrentPerFrame(const float_X deltaTime) :
    deltaTime(deltaTime)
    {
    }

    template<class FrameType, class BoxJ >
    DINLINE void operator()(FrameType& frame, const int localIdx, BoxJ & jBox)
    {

        PMACC_AUTO(particle, frame[localIdx]);
        const float_X weighting = particle[weighting_];
        const floatD_X pos = particle[position_];
        const int particleCellIdx = particle[localCellIdx_];
        const float_X charge = attribute::getCharge(weighting,particle);
        const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec > (particleCellIdx));

        Velocity velocity;
        const float3_X vel = velocity(
                                      particle[momentum_],
                                      attribute::getMass(weighting,particle));
        PMACC_AUTO(fieldJShiftToParticle, jBox.shift(localCell));
        ParticleAlgo perParticle;
        perParticle(fieldJShiftToParticle,
                    pos,
                    vel,
                    charge,
                    deltaTime
                    );
    }

private:
    const PMACC_ALIGN(deltaTime, float);
};

template<class Mapping>
__global__ void kernelAddCurrentToE(typename FieldE::DataBoxType fieldE,
                                    J_DataBox fieldJ,
                                    Mapping mapper)
{

    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                      * Mapping::SuperCellSize::toRT()
                                      );
    const DataSpace<Mapping::Dim> cell(blockCell + DataSpace<simDim > (threadIdx));

    // Amperes Law:
    //   Change of the dE = - j / EPS0 * dt
    //                        j = current density (= current per area)
    //                          = fieldJ
    const float_X deltaT = DELTA_T;
    fieldE(cell) -= fieldJ(cell) * (float_X(1.0) / EPS0) * deltaT;
}

template<class Mapping>
__global__ void kernelBashCurrent(J_DataBox fieldJ,
                                  J_DataBox targetJ,
                                  DataSpace<simDim> exchangeSize,
                                  DataSpace<simDim> direction,
                                  Mapping mapper)
{
    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                      * Mapping::SuperCellSize::toRT()
                                      );
    const DataSpace<Mapping::Dim> threadIndex(threadIdx);
    const DataSpace<Mapping::Dim> sourceCell(blockCell + threadIndex);

    /*origin in area from local GPU*/
    DataSpace<simDim> nullSourceCell(
                                     mapper.getSuperCellIndex(DataSpace<simDim > ())
                                     * Mapping::SuperCellSize::toRT()
                                     );
    DataSpace<simDim> targetCell(sourceCell - nullSourceCell);

    for (uint32_t d = 0; d < simDim; ++d)
    {
        if (direction[d] == -1)
        {
            if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
            targetCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
        }
        else if ((direction[d] == 1) && (threadIndex[d] >= exchangeSize[d])) return;
    }

    targetJ(targetCell) = fieldJ(sourceCell);
}

template<class Mapping>
__global__ void kernelInsertCurrent(J_DataBox fieldJ,
                                    J_DataBox sourceJ,
                                    DataSpace<simDim> exchangeSize,
                                    DataSpace<simDim> direction,
                                    Mapping mapper)
{
    const DataSpace<Mapping::Dim> threadIndex(threadIdx);
    const DataSpace<simDim> blockCell(
                                      mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx))
                                      * Mapping::SuperCellSize::toRT()
                                      );
    DataSpace<Mapping::Dim> targetCell(blockCell + threadIndex);

    /*origin in area from local GPU*/
    DataSpace<simDim> nullSourceCell(
                                     mapper.getSuperCellIndex(DataSpace<simDim > ())
                                     * Mapping::SuperCellSize::toRT()
                                     );
    DataSpace<simDim> sourceCell(targetCell - nullSourceCell);

    for (uint32_t d = 0; d < simDim; ++d)
    {
        if (direction[d] == 1)
        {
            if (threadIndex[d] < SuperCellSize::toRT()[d] - exchangeSize[d]) return;
            sourceCell[d] -= SuperCellSize::toRT()[d] - exchangeSize[d];
            targetCell[d] -= SuperCellSize::toRT()[d];
        }
        else if (direction[d] == -1)
        {
            if (threadIndex[d] >= exchangeSize[d]) return;
            targetCell[d] += SuperCellSize::toRT()[d];
        }
    }

    fieldJ(targetCell) += sourceJ(sourceCell);
}

}
