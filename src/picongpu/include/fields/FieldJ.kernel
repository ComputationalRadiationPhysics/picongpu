/* Copyright 2013-2017 Axel Huebl, Heiko Burau, Rene Widera, Marco Garten,
 *                     Benjamin Worpitz
 *
 * This file is part of PIConGPU.
 *
 * PIConGPU is free software: you can redistribute it and/or modify
 * it under the terms of the GNU General Public License as published by
 * the Free Software Foundation, either version 3 of the License, or
 * (at your option) any later version.
 *
 * PIConGPU is distributed in the hope that it will be useful,
 * but WITHOUT ANY WARRANTY; without even the implied warranty of
 * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 * GNU General Public License for more details.
 *
 * You should have received a copy of the GNU General Public License
 * along with PIConGPU.
 * If not, see <http://www.gnu.org/licenses/>.
 */

#pragma once

#include "pmacc_types.hpp"
#include "particles/frame_types.hpp"

#include "simulation_defines.hpp"

#include "FieldJ.hpp"
#include "particles/memory/boxes/ParticlesBox.hpp"


#include "algorithms/Velocity.hpp"

#include "memory/boxes/CachedBox.hpp"
#include "dimensions/DataSpaceOperations.hpp"
#include "nvidia/functors/Add.hpp"
#include "mappings/threads/ThreadCollective.hpp"
#include "algorithms/Set.hpp"
#include "mappings/threads/ForEachIdx.hpp"
#include "mappings/threads/IdxConfig.hpp"
#include "memory/CtxArray.hpp"
#include "particles/frame_types.hpp"

namespace picongpu
{

using namespace PMacc;

typedef FieldJ::DataBoxType J_DataBox;

/** compute current
 *
 * @tparam T_numWorkers number of workers
 * @tparam T_BlockDescription current field domain description needed for the
 *                            collective stencil
 */
template<
    uint32_t T_numWorkers,
    typename T_BlockDescription
>
struct KernelComputeCurrent
{
    /** scatter particle current of particles located in a supercell
     *
     * The current for the supercell including the guards is cached in shared memory
     * and scattered at the end of the functor to the global memory.
     *
     * @tparam JBox PMacc::DataBox, particle current box type
     * @tparam ParBox PMacc::ParticlesBox, particle box type
     * @tparam Mapping mapper functor type
     * @tparam FrameSolver frame solver functor type
     *
     * @param fieldJ field with particle current
     * @param boxPar particle memory
     * @param frameSolver functor to calculate the current for a frame
     * @param mapper functor to map a block to a supercell
     */
    template<
        typename JBox,
        typename ParBox,
        typename FrameSolver,
        typename Mapping
    >
    DINLINE void operator()(
        JBox fieldJ,
        ParBox boxPar,
        FrameSolver frameSolver,
        Mapping mapper
    ) const
    {
        using namespace mappings::threads;

        using FrameType = typename ParBox::FrameType;
        using FramePtr = typename ParBox::FramePtr;
        using SuperCellSize = typename Mapping::SuperCellSize;

        /** @todo numParticlesPerFrame should be max number of particles within a frame
         * and not a magic number derived from SuperCellSize
         */
        constexpr uint32_t numParticlesPerFrame = PMacc::math::CT::volume< SuperCellSize >::type::value;
        constexpr uint32_t numWorkers = T_numWorkers;

        /* We work with virtual CUDA blocks if we have more workers than particles.
         * Each virtual CUDA block is working on a frame, if we have 2 blocks each block processes
         * every second frame until all frames are processed.
         */
        constexpr uint32_t numVirtualBlocks = ( numWorkers + numParticlesPerFrame - 1u ) / numParticlesPerFrame;


        const DataSpace< simDim > block(
            mapper.getSuperCellIndex(
                DataSpace< simDim >( blockIdx )
            )
        );
        uint32_t const workerIdx = threadIdx.x;

        using VirtualWorkerDomCfg = IdxConfig<
            numParticlesPerFrame * numVirtualBlocks,
            numWorkers
        >;

        /* each virtual worker is part of one virtual block */
        memory::CtxArray<
            uint32_t,
            VirtualWorkerDomCfg
        >
        virtualBlockIdCtx(
            workerIdx,
            [&](
                uint32_t const linearIdx,
                uint32_t const
            )
            {
                return linearIdx / numParticlesPerFrame;
            }
        );

        /* linear virtual worker index in the virtual block*/
        memory::CtxArray<
            uint32_t,
            VirtualWorkerDomCfg
        >
        virtualLinearIdCtx(
            workerIdx,
            [&](
                uint32_t const linearIdx,
                uint32_t const idx
            )
            {
                /* map virtualLinearIdCtx to the range [0;numParticlesPerFrame) */
                return linearIdx - ( virtualBlockIdCtx[ idx ] * numParticlesPerFrame );
            }
        );

        /* each virtual worker stores the currently used frame */
        memory::CtxArray<
            FramePtr,
            VirtualWorkerDomCfg
        > frameCtx;

        memory::CtxArray<
            lcellId_t,
            VirtualWorkerDomCfg
        > particlesInSuperCellCtx( 0u );

        /* loop over all virtual workers */
        ForEachIdx< VirtualWorkerDomCfg > forEachVirtualWorker( workerIdx );

        forEachVirtualWorker(
            [&](
                uint32_t const,
                uint32_t const idx
            )
            {
                frameCtx[ idx ] = boxPar.getLastFrame( block );
                if( frameCtx[ idx ].isValid() && virtualBlockIdCtx[ idx ] == 0u )
                    particlesInSuperCellCtx[ idx ] = boxPar.getSuperCell( block ).getSizeLastFrame();

                /* select N-th (N=virtualBlockId) frame from the end of the list */
                for( uint32_t i = 1; i <= virtualBlockIdCtx[ idx ] && frameCtx[ idx ].isValid(); ++i )
                {
                    particlesInSuperCellCtx[ idx ] = numParticlesPerFrame;
                    frameCtx[ idx ] = boxPar.getPreviousFrame( frameCtx[ idx ] );
                }
            }
        );

        /* this memory is used by all virtual blocks */
        auto cachedJ = CachedBox::create <
            0u,
            typename JBox::ValueType
        >( T_BlockDescription() );

        Set< typename JBox::ValueType > set( float3_X::create( 0.0 ) );
        ThreadCollective<
            T_BlockDescription,
            numWorkers
        > collectiveSet( workerIdx );

        /* initialize shared memory with zeros */
        collectiveSet( set, cachedJ );

        __syncthreads();

        while( true )
        {
            bool isOneFrameValid = false;
            forEachVirtualWorker(
                [&](
                    uint32_t const,
                    uint32_t const idx
                )
                {
                    isOneFrameValid = isOneFrameValid || frameCtx[ idx ].isValid();
                }
            );

            if( !isOneFrameValid )
                break;

            forEachVirtualWorker(
                [&](
                    uint32_t const,
                    uint32_t const idx
                )
                {
                    /* this test is only important for the last frame
                     * if the frame is not the last one then: `particlesInSuperCell == numParticlesPerFrame`
                     */
                    if(
                        frameCtx[ idx ].isValid() &&
                        virtualLinearIdCtx[ idx ] < particlesInSuperCellCtx[ idx ]
                    )
                    {
                        frameSolver(
                            *frameCtx[ idx ],
                            virtualLinearIdCtx[ idx ],
                            cachedJ
                        );
                    }
                }
            );

            forEachVirtualWorker(
                [&](
                    uint32_t const,
                    uint32_t const idx
                )
                {
                    if( frameCtx[ idx ].isValid() )
                    {
                        particlesInSuperCellCtx[ idx ] = numParticlesPerFrame;
                        for( int i = 0; i < numVirtualBlocks && frameCtx[ idx ].isValid(); ++i )
                        {
                            frameCtx[ idx ] = boxPar.getPreviousFrame( frameCtx[ idx ] );
                        }
                    }
                }
            );
        }

        /* we wait that all workers finish the loop */
        __syncthreads();

        nvidia::functors::Add add;
        DataSpace< simDim > const blockCell = block * SuperCellSize::toRT();
        ThreadCollective<
            T_BlockDescription,
            numWorkers
        > collectiveAdd( workerIdx );
        auto fieldJBlock = fieldJ.shift( blockCell );

        /* write scatter results back to the global memory */
        collectiveAdd(
            add,
            fieldJBlock,
            cachedJ
        );
    }
};

template<class ParticleAlgo, class Velocity, class TVec>
struct ComputeCurrentPerFrame
{

    HDINLINE ComputeCurrentPerFrame(const float_X deltaTime) :
    m_deltaTime(deltaTime)
    {
    }

    template<class FrameType, class BoxJ >
    DINLINE void operator()(FrameType& frame, const int localIdx, BoxJ & jBox)
    {

        auto particle = frame[localIdx];
        const float_X weighting = particle[weighting_];
        const floatD_X pos = particle[position_];
        const int particleCellIdx = particle[localCellIdx_];
        const float_X charge = attribute::getCharge(weighting,particle);
        const DataSpace<simDim> localCell(DataSpaceOperations<simDim>::template map<TVec > (particleCellIdx));

        Velocity velocity;
        const float3_X vel = velocity(
                                      particle[momentum_],
                                      attribute::getMass(weighting,particle));
        auto fieldJShiftToParticle = jBox.shift(localCell);
        ParticleAlgo perParticle;
        perParticle(fieldJShiftToParticle,
                    pos,
                    vel,
                    charge,
                    m_deltaTime
                    );
    }

private:
    const PMACC_ALIGN(m_deltaTime, float_32);
};

struct KernelAddCurrentToEMF
{
    template<class T_CurrentInterpolation, class T_Mapping>
    DINLINE void operator()(typename FieldE::DataBoxType fieldE,
                                          typename FieldB::DataBoxType fieldB,
                                          J_DataBox fieldJ,
                                          T_CurrentInterpolation currentInterpolation,
                                          T_Mapping mapper) const
    {
        /* Caching of fieldJ */
        typedef SuperCellDescription<
                    SuperCellSize,
                    typename T_CurrentInterpolation::LowerMargin,
                    typename T_CurrentInterpolation::UpperMargin
                    > BlockArea;

        auto cachedJ = CachedBox::create < 0, typename J_DataBox::ValueType > (BlockArea());

        nvidia::functors::Assign assign;
        const DataSpace<simDim> block(mapper.getSuperCellIndex(DataSpace<simDim > (blockIdx)));
        const DataSpace<simDim> blockCell = block * MappingDesc::SuperCellSize::toRT();

        const DataSpace<simDim > threadIndex(threadIdx);
        auto fieldJBlock = fieldJ.shift(blockCell);

        ThreadCollective<BlockArea> collective(threadIndex);
        collective(
                  assign,
                  cachedJ,
                  fieldJBlock
                  );

        __syncthreads();

        const DataSpace<T_Mapping::Dim> cell(blockCell + threadIndex);

        // Amperes Law:
        //   Change of the dE = - j / EPS0 * dt
        //                        j = current density (= current per area)
        //                          = fieldJ
        currentInterpolation( fieldE.shift(cell), fieldB.shift(cell), cachedJ.shift(threadIndex) );
    }
};

/** copy the current from the local to the intermediate buffer
 *
 * @tparam T_numWorkers number of workers
 */
template< uint32_t T_numWorkers >
struct KernelBashCurrent
{
    template< typename Mapping >
    DINLINE void operator()(
        J_DataBox const & fieldJ,
        J_DataBox & targetJ,
        DataSpace<simDim> const & exchangeSize,
        DataSpace<simDim> const & direction,
        Mapping mapper
    ) const
    {
        using namespace mappings::threads;

        /* number of cells in a superCell */
        constexpr uint32_t numCells = PMacc::math::CT::volume< SuperCellSize >::type::value;
        constexpr uint32_t numWorkers = T_numWorkers;

        uint32_t const workerIdx = threadIdx.x;

        DataSpace< simDim > const blockCell(
            mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) )
                * Mapping::SuperCellSize::toRT()
        );

        /*origin in area from local GPU*/
        DataSpace< simDim > nullSourceCell(
            mapper.getSuperCellIndex( DataSpace< simDim > () )
                * Mapping::SuperCellSize::toRT()
        );

        ForEachIdx<
            IdxConfig<
                numCells,
                numWorkers
            >
        >{ workerIdx }(
            [&](
                uint32_t const linearIdx,
                uint32_t const
            )
            {
                /* cell index within the superCell */
                DataSpace< simDim > const cellIdx = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );

                DataSpace<Mapping::Dim> const sourceCell( blockCell + cellIdx );
                DataSpace<simDim> targetCell( sourceCell - nullSourceCell );

                bool assignValue = true;

                for( uint32_t d = 0; d < simDim; ++d )
                {
                    if( direction[ d ] == -1 )
                    {
                        if( cellIdx[ d ] < SuperCellSize::toRT()[ d ] - exchangeSize[ d ] )
                            assignValue = false;
                        targetCell[ d ] -= SuperCellSize::toRT()[ d ] - exchangeSize[ d ];
                    }
                    else if( direction[d] == 1 && cellIdx[ d ] >= exchangeSize[d]  )
                        assignValue = false;
                }

                if( assignValue )
                    targetJ( targetCell ) = fieldJ( sourceCell );
            }
        );
    }
};

/** add the current from the intermediate to the local buffer
 *
 * @tparam T_numWorkers number of workers
 */
template< uint32_t T_numWorkers >
struct KernelInsertCurrent
{
    template< typename  Mapping >
    DINLINE void operator()(
        J_DataBox & fieldJ,
        J_DataBox const & sourceJ,
        DataSpace<simDim> const & exchangeSize,
        DataSpace<simDim> const & direction,
        Mapping mapper
    ) const
    {
        using namespace mappings::threads;

        /* number of cells in a superCell */
        constexpr uint32_t numCells = PMacc::math::CT::volume< SuperCellSize >::type::value;
        constexpr uint32_t numWorkers = T_numWorkers;

        uint32_t const workerIdx = threadIdx.x;

        DataSpace< simDim > const  blockCell(
            mapper.getSuperCellIndex( DataSpace< simDim >( blockIdx ) )
                * Mapping::SuperCellSize::toRT()
        );

        /*origin in area from local GPU*/
        DataSpace< simDim > nullSourceCell(
            mapper.getSuperCellIndex( DataSpace< simDim > () )
            * Mapping::SuperCellSize::toRT()
        );

        ForEachIdx<
            IdxConfig<
                numCells,
                numWorkers
            >
        >{ workerIdx }(
            [&](
                uint32_t const linearIdx,
                uint32_t const
            )
            {
                /* cell index within the superCell */
                DataSpace< simDim > const cellIdx = DataSpaceOperations< simDim >::template map< SuperCellSize >( linearIdx );
                DataSpace< simDim > targetCell( blockCell + cellIdx );
                DataSpace< simDim > sourceCell( targetCell - nullSourceCell );

                bool assignValue = true;

                for( uint32_t d = 0; d < simDim; ++d )
                {
                    if( direction[ d ] == 1 )
                    {
                        if( cellIdx[ d ] < SuperCellSize::toRT()[ d ] - exchangeSize[ d ] )
                            assignValue = false;
                        sourceCell[ d ] -= SuperCellSize::toRT()[ d ] - exchangeSize[ d ];
                        targetCell[ d ] -= SuperCellSize::toRT()[ d ];
                    }
                    else if( direction[ d ] == -1 )
                    {
                        if( cellIdx[ d ] >= exchangeSize[ d ] )
                            assignValue = false;
                        targetCell[ d ] += SuperCellSize::toRT()[ d ];
                    }
                }
                if( assignValue )
                    fieldJ( targetCell ) += sourceJ( sourceCell );
            }
        );
    }
};

} // namespace picongpu
